{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPdPwUUM7SzmZoHIcLDZuld",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bd7489051cf744fc9917226f34185d7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94aba8e0174647aa97dcaab53cf0b8db",
              "IPY_MODEL_0d5b765db0854e81a84049756c93716e",
              "IPY_MODEL_ad709aaf33264919b9b2f62e68bd6d6e"
            ],
            "layout": "IPY_MODEL_5ae1cd03501544a1b1df6a1ce1bf4afc"
          }
        },
        "94aba8e0174647aa97dcaab53cf0b8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d046b7eeb1847fb8d95a1e1cc6a2aa3",
            "placeholder": "​",
            "style": "IPY_MODEL_0fdaa23ede4c4d75b8c79fa5351f1277",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0d5b765db0854e81a84049756c93716e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a7620ca1fdb4ccf83b18c0b53be8d4c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eed68515fdc34b129ccfcf7e20926732",
            "value": 2
          }
        },
        "ad709aaf33264919b9b2f62e68bd6d6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb7915af3c5a4946ade11856e71b9c92",
            "placeholder": "​",
            "style": "IPY_MODEL_6ceb1ee8258049f48655e5ee45f134c0",
            "value": " 2/2 [00:16&lt;00:00,  7.65s/it]"
          }
        },
        "5ae1cd03501544a1b1df6a1ce1bf4afc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d046b7eeb1847fb8d95a1e1cc6a2aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fdaa23ede4c4d75b8c79fa5351f1277": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a7620ca1fdb4ccf83b18c0b53be8d4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eed68515fdc34b129ccfcf7e20926732": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb7915af3c5a4946ade11856e71b9c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ceb1ee8258049f48655e5ee45f134c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenqiglantz/edd-recursive-doc-agent-vs-metadata-replacement/blob/main/edd_zephyr_7b_gpt3_5_metadata_replacement_multi_doc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Driven Development for Multi Document RAG Pipeline with GPT-3.5 and Zephyr-7b\n",
        "\n",
        "This notebook demonstrates how to use EDD to decide which of the two LLMs perform best for a multi document RAG pipeline for Metadata replacement + node sentence window:\n",
        "\n",
        "\n",
        "*   gpt-3.5-turbo\n",
        "*   zephyr-7b-alpha\n",
        "\n",
        "Suggest to upgrade to Colab Pro to run on T4 high-RAM. I tried to run on the free tier T4 GPU but failed during the download of Zephyr-7b.\n"
      ],
      "metadata": {
        "id": "YTyvoKy7zbUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index==0.8.45.post1 pypdf sentence-transformers transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZhtMcUkhCRn",
        "outputId": "90eb2e1f-f5d1-4b4d-9ee9-5681ef65e521"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama_index==0.8.45.post1 in /usr/local/lib/python3.10/dist-packages (0.8.45.post1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (3.16.4)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.41.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (2.0.21)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (0.5.14)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (2023.6.0)\n",
            "Requirement already satisfied: langchain>=0.0.303 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (0.0.314)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (1.23.5)\n",
            "Requirement already satisfied: openai>=0.26.4 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (0.28.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (1.5.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (8.2.3)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (0.5.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (4.5.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (0.9.0)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from llama_index==0.8.45.post1) (1.26.17)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->llama_index==0.8.45.post1) (3.20.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama_index==0.8.45.post1) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama_index==0.8.45.post1) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama_index==0.8.45.post1) (4.0.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama_index==0.8.45.post1) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama_index==0.8.45.post1) (0.0.43)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama_index==0.8.45.post1) (1.10.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index==0.8.45.post1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama_index==0.8.45.post1) (1.3.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama_index==0.8.45.post1) (3.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (17.0.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama_index==0.8.45.post1) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index==0.8.45.post1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama_index==0.8.45.post1) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index==0.8.45.post1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index==0.8.45.post1) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index==0.8.45.post1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index==0.8.45.post1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama_index==0.8.45.post1) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama_index==0.8.45.post1) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama_index==0.8.45.post1) (1.1.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama_index==0.8.45.post1) (2.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama_index==0.8.45.post1) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load documents"
      ],
      "metadata": {
        "id": "yoLvyqElVEYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "titles = [\n",
        "    \"DevOps_Self-Service_Pipeline_Architecture\",\n",
        "    \"DevOps_Self-Service_Terraform_Project_Structure\",\n",
        "    \"DevOps_Self-Service_Pipeline_Security_Guardrails\"\n",
        "    ]\n",
        "\n",
        "documents = {}\n",
        "for title in titles:\n",
        "    documents[title] = SimpleDirectoryReader(input_files=[f\"./data/{title}.pdf\"]).load_data()\n",
        "print(f\"loaded documents with {len(documents)} documents\")"
      ],
      "metadata": {
        "id": "YSe_WBU9j3ns",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "195b9ed9-a38c-4fbd-8c87-f2de2e4f1d75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded documents with 3 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up node parser, service context"
      ],
      "metadata": {
        "id": "yOHpI6ukuPUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import ServiceContext, set_global_service_context\n",
        "from llama_index.llms import OpenAI\n",
        "from llama_index.embeddings import OpenAIEmbedding, HuggingFaceEmbedding\n",
        "from llama_index.node_parser import SentenceWindowNodeParser, SimpleNodeParser\n",
        "\n",
        "# create the sentence window node parser\n",
        "node_parser = SentenceWindowNodeParser.from_defaults(\n",
        "    window_size=3,\n",
        "    window_metadata_key=\"window\",\n",
        "    original_text_metadata_key=\"original_text\",\n",
        ")\n",
        "simple_node_parser = SimpleNodeParser.from_defaults()"
      ],
      "metadata": {
        "id": "8oVgXUbnzYO8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## on gpt-3.5-turbo"
      ],
      "metadata": {
        "id": "QPARPBc4ZiBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract nodes and build index"
      ],
      "metadata": {
        "id": "V99bBPPoud9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, openai, logging, sys\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-##############\"\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)"
      ],
      "metadata": {
        "id": "9WMxhw2Yjpjs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define LLM and embedding model\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
        "ctx = ServiceContext.from_defaults(\n",
        "    llm=llm,\n",
        "    embed_model=\"local:BAAI/bge-base-en-v1.5\"\n",
        ")\n",
        "\n",
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "# extract nodes and build index\n",
        "document_list = SimpleDirectoryReader(\"data\").load_data()\n",
        "nodes = node_parser.get_nodes_from_documents(document_list)\n",
        "sentence_index = VectorStoreIndex(nodes, service_context=ctx)"
      ],
      "metadata": {
        "id": "An74pufeQSQV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define query engine"
      ],
      "metadata": {
        "id": "qLSEFZGZYBrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
        "\n",
        "metadata_query_engine = sentence_index.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    # the target key defaults to `window` to match the node_parser's default\n",
        "    node_postprocessors=[\n",
        "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "ZqQEKsHYQW4p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run test queries"
      ],
      "metadata": {
        "id": "pBcK9d-sYYeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = metadata_query_engine.query(\"Give me a summary of DevOps self-service-centric pipeline security and guardrails.\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxUa5-t7Qr-5",
        "outputId": "9b190b79-6101-4d85-c3fa-dd181fd3218c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DevOps self-service-centric pipeline security and guardrails involve implementing a set of actions to ensure the security of pipelines, infrastructure, source code, base images, and dependent libraries. These actions are hand-picked and aim to provide security scans and guardrails for various components of the DevOps process. The goal is to establish a secure and reliable self-service environment for DevOps practices.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = metadata_query_engine.query(\"What is Harden Runner in DevOps self-service-centric pipeline security and guardrails?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hspa0axrRFlf",
        "outputId": "dd7324e2-b8e8-4d98-beb9-18eb8979c67f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harden-Runner is a purpose-built security monitoring agent that is used in all pipelines, including infrastructure and application pipelines for both CI and CD workflows. It automatically discovers and correlates outbound traffic with each step in the pipeline to detect and prevent malicious patterns. Its main purpose is to prevent the exfiltration of credentials in the pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## zephyr-7b"
      ],
      "metadata": {
        "id": "CH_ujQmGC47N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)"
      ],
      "metadata": {
        "id": "xiaxcVbTaHiu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "from llama_index.prompts import PromptTemplate\n",
        "from llama_index.llms import HuggingFaceLLM\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "\n",
        "def messages_to_prompt(messages):\n",
        "  prompt = \"\"\n",
        "  for message in messages:\n",
        "    if message.role == 'system':\n",
        "      prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'user':\n",
        "      prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
        "    elif message.role == 'assistant':\n",
        "      prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
        "\n",
        "  # ensure we start with a system prompt, insert blank if needed\n",
        "  if not prompt.startswith(\"<|system|>\\n\"):\n",
        "    prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
        "\n",
        "  # add final assistant prompt\n",
        "  prompt = prompt + \"<|assistant|>\\n\"\n",
        "\n",
        "  return prompt\n",
        "\n",
        "\n",
        "llm_zephyr = HuggingFaceLLM(\n",
        "    model_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    tokenizer_name=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "    query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
        "    context_window=3900,\n",
        "    max_new_tokens=256,\n",
        "    model_kwargs={\"quantization_config\": quantization_config},\n",
        "    # tokenizer_kwargs={},\n",
        "    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "bd7489051cf744fc9917226f34185d7c",
            "94aba8e0174647aa97dcaab53cf0b8db",
            "0d5b765db0854e81a84049756c93716e",
            "ad709aaf33264919b9b2f62e68bd6d6e",
            "5ae1cd03501544a1b1df6a1ce1bf4afc",
            "2d046b7eeb1847fb8d95a1e1cc6a2aa3",
            "0fdaa23ede4c4d75b8c79fa5351f1277",
            "6a7620ca1fdb4ccf83b18c0b53be8d4c",
            "eed68515fdc34b129ccfcf7e20926732",
            "fb7915af3c5a4946ade11856e71b9c92",
            "6ceb1ee8258049f48655e5ee45f134c0"
          ]
        },
        "id": "BaKJ6ytnZvyp",
        "outputId": "f5b11bd9-4f42-4a3f-aac9-ce525fe7f1cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd7489051cf744fc9917226f34185d7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from llama_index import ServiceContext\n",
        "\n",
        "service_context_zephyr = ServiceContext.from_defaults(\n",
        "    llm=llm_zephyr,\n",
        "    embed_model=\"local:BAAI/bge-base-en-v1.5\"\n",
        ")"
      ],
      "metadata": {
        "id": "cZTJE9-d4vex"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract nodes and build index"
      ],
      "metadata": {
        "id": "-Z0aWGmRYxwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "\n",
        "document_list = SimpleDirectoryReader(\"data\").load_data()\n",
        "nodes = node_parser.get_nodes_from_documents(document_list)\n",
        "sentence_index_zephyr = VectorStoreIndex(nodes, service_context=service_context_zephyr)"
      ],
      "metadata": {
        "id": "cXbNk40sauoy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define query engine"
      ],
      "metadata": {
        "id": "CJnUQ35ebQdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.indices.postprocessor import MetadataReplacementPostProcessor\n",
        "\n",
        "metadata_query_engine_zephyr = sentence_index_zephyr.as_query_engine(\n",
        "    similarity_top_k=2,\n",
        "    # the target key defaults to `window` to match the node_parser's default\n",
        "    node_postprocessors=[\n",
        "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "KkP5LWs2bSJk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run test queries"
      ],
      "metadata": {
        "id": "Zd1c8by0ZRJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Give me a summary of DevOps self-service-centric pipeline security and guardrails.\"\n",
        "response = metadata_query_engine_zephyr.query(query)\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "GL2cEx1FDfQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59493c8d-5d48-485c-f62d-c38f1b8741ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The article discusses DevOps self-service-centric pipeline security and guardrails, providing a list of hand-picked actions for security scans and guardrails for pipelines, infrastructure, source code, base images, and dependent libraries. The author acknowledges that coming from a traditional DevOps mindset, security measures and guardrails may be a concern. The article is part of a series that explores DevOps self-service pipeline architecture, Terraform project structure, and GitHub Actions workflow orchestration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Harden Runner in DevOps self-service-centric pipeline security and guardrails?\"\n",
        "response = metadata_query_engine_zephyr.query(query)\n",
        "print(str(response))"
      ],
      "metadata": {
        "id": "eRlbdpYCFwlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75d1e7b-4d70-46ea-e17b-cec60879fa4b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harden Runner is a purpose-built security monitoring agent for pipelines that automatically discovers and correlates outbound traffic with each step in the pipeline to detect and prevent malicious patterns observed during past software supply chain security breaches. It is used in all pipelines, including infrastructure and application pipelines for CI and CD, and is the only action used in all pipelines due to its unique nature and purpose. Its main features include preventing exfiltration of credentials in the pipeline.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluations"
      ],
      "metadata": {
        "id": "SVlkfNvMW0pA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate evaluation questions"
      ],
      "metadata": {
        "id": "041LeIE-r5PR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(42)\n",
        "from llama_index.evaluation import DatasetGenerator\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# load data\n",
        "document_list = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "question_dataset = []\n",
        "if os.path.exists(\"question_dataset.txt\"):\n",
        "    with open(\"question_dataset.txt\", encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            question_dataset.append(line.strip())\n",
        "else:\n",
        "    # generate questions\n",
        "    data_generator = DatasetGenerator.from_documents(document_list)\n",
        "    generated_questions = data_generator.generate_questions_from_nodes()\n",
        "    print(f\"Generated {len(generated_questions)} questions.\")\n",
        "\n",
        "    # randomly pick 30 questions\n",
        "    generated_questions = random.sample(generated_questions, 30)\n",
        "    question_dataset.extend(generated_questions)\n",
        "    print(f\"Randomly picked {len(question_dataset)} questions.\")\n",
        "\n",
        "    # save the questions into a txt file\n",
        "    with open(\"question_dataset.txt\", \"w\") as f:\n",
        "        for question in question_dataset:\n",
        "            f.write(f\"{question.strip()}\\n\")\n",
        "\n",
        "for i, question in enumerate(question_dataset, start=1):\n",
        "    print(f\"{i}. {question}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMFJhOCqgwk0",
        "outputId": "4595301b-0c69-42dc-dfda-730a56c1cfed"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. What is the high-level design of DevOps pipelines?\n",
            "2. What is a recently introduced feature in Infracost Cloud?\n",
            "3. What is the purpose of Infracost in cloud cost management?\n",
            "4. Why is it important to include TruffleHog in your pipelines?\n",
            "5. How can you fix the vulnerability in the base image according to the provided instructions?\n",
            "6. What is the purpose of the aquasecurity/trivy-action in the GitHub Actions CI workflow?\n",
            "7. What are the optional parameters that can be used with the Checkov action?\n",
            "8. How can Infracost be integrated into the infrastructure pipeline?\n",
            "9. How are application pipelines triggered?\n",
            "10. Give me a summary of DevOps Self-Service Pipeline Architecture and Its 3–2–1 Rule.\n",
            "11. What command is used to generate the Infracost report in HTML format?\n",
            "12. How does Terraform enable the creation of reusable infrastructure?\n",
            "13. How can the GitHub Actions workflow be configured to dynamically select the backend configuration file based on the environment?\n",
            "14. What is the diff feature in Infracost and how does it serve as a guardrail for cloud cost management?\n",
            "15. How does Infracost provide a safety net for catching abnormal cloud cost estimates?\n",
            "16. What is the purpose of uploading the report to an artifact?\n",
            "17. What types of files or systems can Trivy scan?\n",
            "18. What severity levels does Trivy consider for vulnerabilities?\n",
            "19. What is the purpose of the Terraform GitHub Actions workflow?\n",
            "20. Can you provide a link to a website that provides information on creating Terraform modules?\n",
            "21. What is the intended audience for these documents?\n",
            "22. What is the purpose of the \"DevOps Self-Service -Centric GitHub Actions’ Workflow Orchestration\" article?\n",
            "23. What is the purpose of the \"--soft-fail\" flag in the TFSec step of the workflow?\n",
            "24. Give me a summary of DevOps self-service-centric pipeline security and guardrails.\n",
            "25. What command is used to initialize Terraform with a specific backend configuration file?\n",
            "26. What information can Trivy find during a scan?\n",
            "27. What CLI configuration files should be ignored?\n",
            "28. What is the purpose of Terraform as an Infrastructure as Code (IaC) tool?\n",
            "29. What is SonarScan and what does it analyze in source code?\n",
            "30. Give me a summary of DevOps Self-Service Centric Terraform Project Structure.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define evaluators"
      ],
      "metadata": {
        "id": "Cbouoes9r1M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.evaluation import FaithfulnessEvaluator, RelevancyEvaluator\n",
        "\n",
        "# use gpt-4 to evaluate\n",
        "gpt4_service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0.1, llm=\"gpt-4\"))\n",
        "\n",
        "faithfulness_gpt4 = FaithfulnessEvaluator(service_context=gpt4_service_context)\n",
        "relevancy_gpt4 = RelevancyEvaluator(service_context=gpt4_service_context)"
      ],
      "metadata": {
        "id": "LtE78hi_hlzs"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define evaluation batch runner"
      ],
      "metadata": {
        "id": "eGrT4R3Wr9cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.evaluation import BatchEvalRunner\n",
        "\n",
        "runner = BatchEvalRunner(\n",
        "    {\"faithfulness\": faithfulness_gpt4, \"relevancy\": relevancy_gpt4},\n",
        "    workers=10,\n",
        "    show_progress=True\n",
        ")"
      ],
      "metadata": {
        "id": "IdvoSHfczYLA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_eval_results(key, eval_results):\n",
        "    results = eval_results[key]\n",
        "    correct = 0\n",
        "    for result in results:\n",
        "        if result.passing:\n",
        "            correct += 1\n",
        "    score = correct / len(results)\n",
        "    print(f\"{key} Correct: {correct}. Score: {score}\")\n",
        "    return score"
      ],
      "metadata": {
        "id": "u06cC3F3srKx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on gpt-3.5-turbo"
      ],
      "metadata": {
        "id": "FFS4nzsvsCF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = await runner.aevaluate_queries(\n",
        "    metadata_query_engine, queries=question_dataset\n",
        ")\n",
        "\n",
        "print(\"------------------\")\n",
        "score = get_eval_results(\"faithfulness\", eval_results)\n",
        "score = get_eval_results(\"relevancy\", eval_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGryhDAQnE8X",
        "outputId": "19a23848-c0ad-4959-f082-c894b5a43751"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:05<00:00,  5.08it/s]\n",
            "100%|██████████| 60/60 [00:02<00:00, 22.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------\n",
            "faithfulness Correct: 28. Score: 0.9333333333333333\n",
            "relevancy Correct: 27. Score: 0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on zephyr-7b"
      ],
      "metadata": {
        "id": "I0IvBdjmsIUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = await runner.aevaluate_queries(\n",
        "    metadata_query_engine_zephyr, queries=question_dataset\n",
        ")\n",
        "\n",
        "print(\"------------------\")\n",
        "score = get_eval_results(\"faithfulness\", eval_results)\n",
        "score = get_eval_results(\"relevancy\", eval_results)"
      ],
      "metadata": {
        "id": "Vc3PmNxKD-Dr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7af0e67d-c6ee-40af-bcab-a6c40e22d291"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [04:32<00:00,  9.07s/it]  \n",
            "100%|██████████| 60/60 [00:02<00:00, 20.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------\n",
            "faithfulness Correct: 28. Score: 0.9333333333333333\n",
            "relevancy Correct: 29. Score: 0.9666666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}